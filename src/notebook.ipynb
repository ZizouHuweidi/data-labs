{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Insults in Social Commentary\n",
    "\n",
    "## 1. Business Understanding\n",
    "\n",
    "### 1.1 Project description\n",
    "Analyzing data related to insults found in social commentary to provide valuable insights into the use of language, sentiment, and discourse patterns in online discussions. \n",
    "\n",
    "Predict whether a comment posted during a public discussion is considered insulting to one of the participants.\n",
    "\n",
    "### 1.2 Project objectives\n",
    "- To provide valuable insights into the use of language, sentiment, and discourse patterns in online discussions.\n",
    "- To classify into insults and not insults\n",
    "- To conduct a basic frequency analysis to identify the most common insults used in social commentary. This can help pinpoint which insults are more prevalent.\n",
    "\n",
    "## 2. Data Understanding\n",
    "\n",
    "### 2.1. Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "### 2.2. Data exploration\n",
    "\n",
    "df.head(20)\n",
    "\n",
    "### 2.2.1 Columns description\n",
    "\n",
    "# - insults: two possible outputs (0, 1). 0 means no insults, 1 means it contains insults (based on the presence of insult words, not accurate)\n",
    "# - date: the date of the comment\n",
    "# - comment: the comment being analyzed\n",
    "\n",
    "num = df['Insult'].value_counts()\n",
    "\n",
    "print(num)\n",
    "\n",
    "plot = pd.DataFrame({'Comments':['Insults', 'No Insults'], 'val':[num[0], num[1]]})\n",
    "ax = plot.plot.bar(title='Number of insults',x='Comments', y='val', rot=0)\n",
    "\n",
    "## 3. Data preparation and pre-processing\n",
    "\n",
    "### 3.1. Data cleaning\n",
    "\n",
    "#### Clean the data, only show data with insults\n",
    "\n",
    "df_1 = df.dropna()\n",
    "filtered_df = df_1.query('Insult!=0')\n",
    "filtered_df.head(20)\n",
    "\n",
    "### 3.2. Data visualization\n",
    "\n",
    "#### Visualise the data in a wordcloud\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "text = filtered_df['Comment'].to_string(index=False)\n",
    "processed_text = re.sub(r'\\bxa0\\b', '', text)\n",
    "\n",
    "wordcloud = WordCloud(max_words=25, min_word_length=3).generate(processed_text)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.show()\n",
    "\n",
    "# Create a document-term matrix\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(reduced_data['Comment'])\n",
    "vocab = vectorizer.get_feature_names()\n",
    "df = pd.DataFrame(dtm.toarray(), columns=vocab)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm')\n",
    "\n",
    "### 3.3 Data reduction\n",
    "### using random sampling\n",
    "\n",
    "reduced_data = filtered_df.sample(100)\n",
    "reduced_data.head(20)\n",
    "\n",
    "### 3.4. Data transformation\n",
    "\n",
    "# Download the required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Tokenize the text\n",
    "\n",
    "token_arr = []\n",
    "\n",
    "for text in filtered_df['Comment'].values:\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lowercase the tokens\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    tokens = [re.sub(r'[^a-zA-Z0-9]', '', token) for token in tokens]\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    tokens = list(filter(None, tokens))\n",
    "    token_arr.append(tokens)\n",
    "# Print the processed tokens\n",
    "print(token_arr)\n",
    "\n",
    "### 3.5. Feature selection\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(filtered_df[\"Comment\"])\n",
    "print(tfidf_matrix)\n",
    "\n",
    "text = filtered_df[\"Comment\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for i in range(len(text)):\n",
    "    print(\"Document {}\".format(i+1))\n",
    "    for j in range(len(feature_names)):\n",
    "        print(\"{}: {}\".format(feature_names[j], tfidf_matrix[i,j]))\n",
    "\n",
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
